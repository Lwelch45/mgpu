
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Tutorial &mdash; mgpu v0.2 alpha documentation</title>
    <link rel="stylesheet" href="_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '',
        VERSION:     '0.2 alpha',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="top" title="mgpu v0.2 alpha documentation" href="index.html" />
    <link rel="next" title="Rationale" href="rationale.html" />
    <link rel="prev" title="Getting Started" href="gettingstarted.html" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="rationale.html" title="Rationale"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="gettingstarted.html" title="Getting Started"
             accesskey="P">previous</a> |</li>
        <li><a href="index.html">mgpu v0.2 alpha documentation</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
          <div class="body">
            
  <div class="section" id="tutorial">
<h1>Tutorial<a class="headerlink" href="#tutorial" title="Permalink to this headline">¶</a></h1>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><a class="reference internal" href="#containers-and-data-transfer" id="id1">Containers and Data Transfer</a></li>
<li><a class="reference internal" href="#invoking-kernels" id="id2">Invoking Kernels</a></li>
<li><a class="reference internal" href="#synchronization" id="id3">Synchronization</a></li>
<li><a class="reference internal" href="#running-functions-in-device-contexts" id="id4">Running Functions in Device Contexts</a></li>
<li><a class="reference internal" href="#advanced-data-transfer" id="id5">Advanced Data Transfer</a></li>
<li><a class="reference internal" href="#distributed-batched-ffts" id="id6">Distributed Batched FFTs</a></li>
</ul>
</div>
<p>A MGPU program deals with multiple GPUs in one system. To manage and assign work
to each GPU, MGPU creates one thread for each device. Thus to get started, a
runtime environment <tt class="docutils literal"><span class="pre">mgpu::environment</span></tt> should be initialized first. This is
not mandatory: some of the functionality of the MGPU library can be used from a
single thread. For many of the advanced features of the library it is a
requirement to initialize an environment.</p>
<p>By default the environment utilizes all available devices. However, you might
only want to use a subset of all available GPUs in the system. The
environment constructor allows you to do just that, as it accepts a
<tt class="docutils literal"><span class="pre">mgpu::dev_group</span></tt> object that specifies which devices should be used.</p>
<div class="highlight-c++"><div class="highlight"><pre><span class="cp">#include &lt;mgpu/environment.hpp&gt;</span>
<span class="cp">#include &lt;mgpu/backend.hpp&gt;</span>

<span class="k">using</span> <span class="k">namespace</span> <span class="n">mgpu</span><span class="p">;</span>

<span class="kt">int</span> <span class="n">main</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
  <span class="p">{</span>
    <span class="c1">// all available devices</span>
    <span class="n">environment</span> <span class="n">e</span><span class="p">;</span>
  <span class="p">}</span>

  <span class="kt">int</span> <span class="n">devices</span> <span class="o">=</span> <span class="n">backend</span><span class="o">::</span><span class="n">get_dev_count</span><span class="p">();</span>
  <span class="k">if</span><span class="p">(</span><span class="n">devices</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span>
  <span class="p">{</span>
    <span class="c1">// explicitly specify all available devices</span>
    <span class="n">environment</span> <span class="n">e</span><span class="p">(</span><span class="n">dev_group</span><span class="o">::</span><span class="n">from_to</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">devices</span><span class="p">));</span>
  <span class="p">}</span>

  <span class="k">if</span><span class="p">(</span><span class="n">devices</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">)</span>
  <span class="p">{</span>
    <span class="c1">// use devices 0, 1 and 2</span>
    <span class="n">environment</span> <span class="n">e</span><span class="p">(</span><span class="n">dev_group</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">));</span>
  <span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Since it is imperative that the <tt class="docutils literal"><span class="pre">mgpu::environment</span></tt> instance outlives any
other object that relies on the environment, it is sensible to put all
subsequent code in a separate scope.</p>
<p>Wrappers around vendor API functions a located in the <tt class="docutils literal"><span class="pre">mgpu::backend</span></tt>
namespace. While still incomplete, these wrappers aim to present a unified API
to different vendor interfaces such as CUDA and OpenCL.</p>
<div class="section" id="containers-and-data-transfer">
<h2><a class="toc-backref" href="#id1">Containers and Data Transfer</a><a class="headerlink" href="#containers-and-data-transfer" title="Permalink to this headline">¶</a></h2>
<p>A major task when dealing with GPU systems and especially multi-GPU systems is
communication: moving data between host and the different device memories can
quickly become confusing. To simplify this task the MGPU library provides a set
of containers and compatible transfer functions.</p>
<div class="section" id="simple-device-vector">
<h3>Simple Device Vector<a class="headerlink" href="#simple-device-vector" title="Permalink to this headline">¶</a></h3>
<p>The following example shows a simple use of MGPU containers from a single
thread. Two vectors are allocated on different devices (if possible). The device
is switched temporarily for a scope using an instance of the
<tt class="docutils literal"><span class="pre">mgpu::dev_set_scoped</span></tt> class.</p>
<div class="highlight-c++"><div class="highlight"><pre><span class="cp">#include &lt;vector&gt;</span>
<span class="cp">#include &lt;mgpu/container/dev_vector.hpp&gt;</span>
<span class="cp">#include &lt;mgpu/backend.hpp&gt;</span>
<span class="cp">#include &lt;mgpu/transfer/copy.hpp&gt;</span>

<span class="k">using</span> <span class="k">namespace</span> <span class="n">mgpu</span><span class="p">;</span>

<span class="kt">int</span> <span class="n">main</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
  <span class="c1">// allocate memory</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">host_in</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mf">42.</span><span class="p">),</span> <span class="n">host_out</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mf">0.</span><span class="p">);</span>
  <span class="n">dev_vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">dev</span><span class="p">(</span><span class="mi">3</span><span class="p">);</span>

  <span class="c1">// copy to device</span>
  <span class="n">copy</span><span class="p">(</span><span class="n">host_in</span><span class="p">,</span> <span class="n">dev</span><span class="p">.</span><span class="n">begin</span><span class="p">());</span>

  <span class="c1">// if more than 1 device: host -&gt; device0 -&gt; device1 -&gt; host</span>
  <span class="kt">int</span> <span class="n">devices</span> <span class="o">=</span> <span class="n">backend</span><span class="o">::</span><span class="n">get_dev_count</span><span class="p">();</span>
  <span class="k">if</span><span class="p">(</span><span class="n">devices</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span>
  <span class="p">{</span>
    <span class="n">dev_set_scoped</span> <span class="n">s</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
    <span class="n">dev_vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">dev2</span><span class="p">(</span><span class="mi">3</span><span class="p">);</span>
    <span class="c1">// copy from device0 to device1 and back to host</span>
    <span class="n">copy</span><span class="p">(</span><span class="n">dev</span><span class="p">,</span> <span class="n">dev2</span><span class="p">.</span><span class="n">begin</span><span class="p">());</span>
    <span class="n">copy</span><span class="p">(</span><span class="n">dev2</span><span class="p">,</span> <span class="n">host_out</span><span class="p">.</span><span class="n">begin</span><span class="p">());</span>
  <span class="p">}</span>
  <span class="k">else</span> <span class="c1">// if not: host -&gt; device -&gt; host</span>
  <span class="p">{</span>
    <span class="n">copy</span><span class="p">(</span><span class="n">dev</span><span class="p">,</span> <span class="n">host_out</span><span class="p">.</span><span class="n">begin</span><span class="p">());</span>
  <span class="p">}</span>

  <span class="n">printf</span><span class="p">(</span><span class="s">&quot;in %f %f %f | out %f %f %f</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span> <span class="n">host_in</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">host_in</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
    <span class="n">host_in</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">host_out</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">host_out</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">host_out</span><span class="p">[</span><span class="mi">2</span><span class="p">]);</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Data is copied from host memory to device memory, from there to another device
and then back to host memory. Notice that the interface of the <tt class="docutils literal"><span class="pre">mgpu::copy</span></tt>
function is always the same, regardless of the arguments that are passed. It
expects an input range and an output iterator. Other types can be adapted to be
compatible with the interface through type traits. The library currently ships
with traits for <tt class="docutils literal"><span class="pre">std::vector</span></tt> as well as <tt class="docutils literal"><span class="pre">boost::numeric::ublas::vector</span></tt>.</p>
</div>
<div class="section" id="segmented-device-vector">
<h3>Segmented Device Vector<a class="headerlink" href="#segmented-device-vector" title="Permalink to this headline">¶</a></h3>
<p>On to a more involved example. The MGPU library draws many ideas from the
<em>Segmented Iterator concept</em>. It fits multi-GPU architectures well since such
systems are composed of segments of discontinuous memory. The most important
class in this regard is the segmented device vector or <tt class="docutils literal"><span class="pre">mgpu::seg_dev_vector</span></tt>.
It enables you to allocate a large vector, cut the vector in slices and
distribute those slices across the devices of your choosing. The following
example illustrates this feature:</p>
<div class="highlight-c++"><div class="highlight"><pre><span class="cp">#include &lt;vector&gt;</span>
<span class="cp">#include &lt;mgpu/container/seg_dev_vector.hpp&gt;</span>
<span class="cp">#include &lt;mgpu/transfer/copy.hpp&gt;</span>
<span class="cp">#include &lt;mgpu/synchronization.hpp&gt;</span>

<span class="k">using</span> <span class="k">namespace</span> <span class="n">mgpu</span><span class="p">;</span>

<span class="kt">int</span> <span class="n">main</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
  <span class="n">environment</span> <span class="n">e</span><span class="p">;</span>
  <span class="p">{</span>
    <span class="c1">// allocate memory</span>
    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">host_in</span><span class="p">(</span><span class="n">e</span><span class="p">.</span><span class="n">size</span><span class="p">(),</span> <span class="mf">42.</span><span class="p">),</span> <span class="n">host_out</span><span class="p">(</span><span class="n">e</span><span class="p">.</span><span class="n">size</span><span class="p">(),</span> <span class="mf">0.</span><span class="p">);</span>
    <span class="n">seg_dev_vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">dev</span><span class="p">(</span><span class="n">e</span><span class="p">.</span><span class="n">size</span><span class="p">());</span>

    <span class="c1">// copy to device and from device</span>
    <span class="n">copy</span><span class="p">(</span><span class="n">host_in</span><span class="p">,</span> <span class="n">dev</span><span class="p">.</span><span class="n">begin</span><span class="p">());</span>
    <span class="n">copy</span><span class="p">(</span><span class="n">dev</span><span class="p">,</span> <span class="n">host_out</span><span class="p">.</span><span class="n">begin</span><span class="p">());</span>
    <span class="n">synchronize_barrier</span><span class="p">();</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span><span class="o">&lt;</span><span class="n">e</span><span class="p">.</span><span class="n">size</span><span class="p">();</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
      <span class="n">printf</span><span class="p">(</span><span class="s">&quot;in: %f out %f</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span> <span class="n">host_in</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">host_out</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
  <span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>A segmented vector <tt class="docutils literal"><span class="pre">dev</span></tt> is allocated - it&#8217;s size is the size of the
environment which is the number of devices in the system. The <tt class="docutils literal"><span class="pre">dev</span></tt> object
is composed of multiple vectors, one vector on each device. The size of each
vector is 1. The <tt class="docutils literal"><span class="pre">mgpu::copy</span></tt> function the fact that a segmented range is
passed and scatters the host vector across all devices. When copying
the data back, the segments are gathered back into one vector on the host.</p>
<p>Since all operations that are executed in the context of an environment are
asynchronous, it is necessary to wait for completion of the copies before the
memory can be accessed. <tt class="docutils literal"><span class="pre">mgpu::synchronize_barrier()</span></tt> helps in this case:</p>
<ul class="simple">
<li>it synchronizes all devices; it blocks until all operations that were
scheduled for a device are finished</li>
<li>it inserts a barrier; no device can continue executing until all other devices
have reaches this barrier</li>
<li>it blocks the calling thread until the previous two conditions are met</li>
</ul>
<p>In this example the vector is distributed equally across all devices. The way
the vector is split can be controlled using a different segmented vector
constructor. A <tt class="docutils literal"><span class="pre">blocksize</span></tt> can be passed as an additional parameter. It
specifies the minimum block size that the splitting algorithm is allowed to cut
the vector in. Local vector size on each device is always be a multiple of the
blocksize.</p>
<div class="highlight-cpp"><div class="highlight"><pre><span class="n">seg_dev_vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">dev</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">8</span><span class="p">);</span>
</pre></div>
</div>
<p>In this example the overall vector is of size 128 and it is split in chunks of
8.</p>
</div>
</div>
<div class="section" id="invoking-kernels">
<h2><a class="toc-backref" href="#id2">Invoking Kernels</a><a class="headerlink" href="#invoking-kernels" title="Permalink to this headline">¶</a></h2>
<p>Copying data back and forth between memories certainly is not all you will want
to do. We hope to simplify this so that you can focus on what matters most:
implementing the algorithms that process the data once they are on the device.
Launching and managing kernels on a multi-GPU system can be tedious. We thus
provide helper functions to simplify this.</p>
<p>The functions <tt class="docutils literal"><span class="pre">mgpu::invoke_kernel()</span></tt> and <tt class="docutils literal"><span class="pre">mgpu::invoke_kernel_all()</span></tt> invoke
user specified functions in the desired device thread and context. The former
invokes a function in one device context (which device rank must be specified as
the last parameter) and the latter invokes a function in all device contexts.
These functions have a couple of features that come in handy when dealing with
multiple GPUs:</p>
<ul class="simple">
<li>if a <tt class="docutils literal"><span class="pre">seg_dev_vector</span></tt> is passed to on of the invoke functions, the function
called through invoke is not passed the entire segmented vector but a
reference to the local vector relevant to the current device; this can be
disabled for situations where a kernel needs to see the entire segmented
vector, for example for peer-to-peer memory access (this is also true for
segmented streams <tt class="docutils literal"><span class="pre">mgpu::seg_dev_stream</span></tt> and segmented iterators
<tt class="docutils literal"><span class="pre">mgpu::seg_dev_iterator</span></tt>)</li>
<li>the device id (hardware) or the device rank (device enumeration in the
environment) can be passed to the function called by invoke</li>
<li>if a function is invoked for multiple devices (<tt class="docutils literal"><span class="pre">mgpu::invoke_kernel_all()</span></tt>)
one of the devices can be selected to carry out a special task</li>
</ul>
<p>The following example shows how a kernel could be called using the invoke
facilities of the MGPU library.</p>
<div class="highlight-c++"><div class="highlight"><pre><span class="cp">#include &lt;mgpu/invoke_kernel.hpp&gt;</span>
<span class="cp">#include &lt;mgpu/container/seg_dev_vector.hpp&gt;</span>
<span class="cp">#include &lt;mgpu/synchronization.hpp&gt;</span>
<span class="cp">#include &lt;mgpu/environment.hpp&gt;</span>

<span class="k">using</span> <span class="k">namespace</span> <span class="n">mgpu</span><span class="p">;</span>

<span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">T</span><span class="o">&gt;</span>
<span class="kt">void</span> <span class="n">kernel_caller</span><span class="p">(</span><span class="n">dev_range</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;</span> <span class="n">dev</span><span class="p">,</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">size_t</span><span class="o">&gt;</span> <span class="o">&amp;</span> <span class="n">vec</span><span class="p">,</span> <span class="n">dev_rank_t</span> <span class="n">rank</span><span class="p">,</span> <span class="kt">bool</span> <span class="n">select</span><span class="p">)</span>
<span class="p">{</span>
  <span class="c1">// here you would call a kernel using backend syntax (e.g. CUDA)</span>
  <span class="n">vec</span><span class="p">[</span><span class="n">rank</span><span class="p">]</span> <span class="o">=</span> <span class="n">dev</span><span class="p">.</span><span class="n">size</span><span class="p">();</span>
  <span class="k">if</span><span class="p">(</span><span class="n">select</span><span class="p">)</span> <span class="n">printf</span><span class="p">(</span><span class="s">&quot;rank %d was selected</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span> <span class="n">rank</span><span class="p">);</span>
<span class="p">}</span>

<span class="kt">int</span> <span class="n">main</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
  <span class="n">environment</span> <span class="n">e</span><span class="p">;</span>
  <span class="p">{</span>
    <span class="n">seg_dev_vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">dev_vec</span><span class="p">(</span><span class="mi">42</span><span class="p">);</span>
    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">size_t</span><span class="o">&gt;</span> <span class="n">sizes</span><span class="p">(</span><span class="n">e</span><span class="p">.</span><span class="n">size</span><span class="p">());</span>
    <span class="n">invoke_kernel_all</span><span class="p">(</span><span class="n">kernel_caller</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">,</span> <span class="n">dev_vec</span><span class="p">,</span> <span class="n">sizes</span><span class="p">,</span>
      <span class="n">pass_dev_rank</span><span class="p">,</span> <span class="n">select_one</span><span class="p">);</span>
    <span class="n">synchronize_barrier</span><span class="p">();</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span><span class="o">&lt;</span><span class="n">e</span><span class="p">.</span><span class="n">size</span><span class="p">();</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
      <span class="n">printf</span><span class="p">(</span><span class="s">&quot;rank %d size %lu</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">sizes</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
  <span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The <tt class="docutils literal"><span class="pre">kernel_caller</span></tt> function is a stub that is invoked in the device thread
and context. It can launch the desired kernel itself on the device. Passing
the <tt class="docutils literal"><span class="pre">seg_dev_vector</span></tt> translates to a dev_range that corresponds to the local
section of the segmented vector on the current device. The <tt class="docutils literal"><span class="pre">pass_dev_rank</span></tt>
object is an indicator to insert each devices rank as an argument. To clarify:
the call to <tt class="docutils literal"><span class="pre">invoke_kernel_all</span></tt> enqueues the <tt class="docutils literal"><span class="pre">kernel_caller</span></tt> function for
each device with device-specific parameters.</p>
</div>
<div class="section" id="synchronization">
<h2><a class="toc-backref" href="#id3">Synchronization</a><a class="headerlink" href="#synchronization" title="Permalink to this headline">¶</a></h2>
<p>The MGPU library is in two ways asynchronous. The first asynchronous layer is
the backend. Most backend-functions are asynchronous and can be synchronized
using <tt class="docutils literal"><span class="pre">synchronize*</span></tt> functions. The second layer is part of the MGPU library.
For each device that is used, the library allocates a thread. Each thread
receives jobs through a separate work-queue that is populated by the main thread
(using copy, invoke etc. functions). <tt class="docutils literal"><span class="pre">barrier*</span></tt> functions can be used for
synchronization on this layer. They synchronize the device work queues and
optionally block the calling thread. There are combinations off the two
functions provided as well.</p>
</div>
<div class="section" id="running-functions-in-device-contexts">
<h2><a class="toc-backref" href="#id4">Running Functions in Device Contexts</a><a class="headerlink" href="#running-functions-in-device-contexts" title="Permalink to this headline">¶</a></h2>
<p>The environment of the MGPU library can not only be used to feed and manage
devices but is also useful to implement multi-threaded CPU algorithms. The
<tt class="docutils literal"><span class="pre">invoke*</span></tt> family of functions allows filling the device work queues without
the special features that are available in the <tt class="docutils literal"><span class="pre">invoke_kernel*</span></tt> functions and
gives simplified access to the MGPU threading functionality.</p>
</div>
<div class="section" id="advanced-data-transfer">
<h2><a class="toc-backref" href="#id5">Advanced Data Transfer</a><a class="headerlink" href="#advanced-data-transfer" title="Permalink to this headline">¶</a></h2>
<p>Apart from the <tt class="docutils literal"><span class="pre">mgpu::copy</span></tt> function, we provide a broadcast function. It
might be desirable to have the exact same vector on all devices. The following
example demonstrates how the broadcast function allows you to clone a simple
vector across all devices.</p>
<div class="highlight-c++"><div class="highlight"><pre><span class="cp">#include &lt;vector&gt;</span>
<span class="cp">#include &lt;mgpu/container/seg_dev_vector.hpp&gt;</span>
<span class="cp">#include &lt;mgpu/transfer/copy.hpp&gt;</span>
<span class="cp">#include &lt;mgpu/transfer/broadcast.hpp&gt;</span>
<span class="cp">#include &lt;mgpu/synchronization.hpp&gt;</span>

<span class="k">using</span> <span class="k">namespace</span> <span class="n">mgpu</span><span class="p">;</span>

<span class="kt">int</span> <span class="n">main</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
  <span class="n">environment</span> <span class="n">e</span><span class="p">;</span>
  <span class="p">{</span>
    <span class="c1">// allocate memory</span>
    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">host_in</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">42.</span><span class="p">),</span> <span class="n">host_out</span><span class="p">(</span><span class="n">e</span><span class="p">.</span><span class="n">size</span><span class="p">(),</span> <span class="mf">0.</span><span class="p">);</span>
    <span class="n">seg_dev_vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">dev</span><span class="p">(</span><span class="n">e</span><span class="p">.</span><span class="n">size</span><span class="p">());</span>

    <span class="c1">// copy to device and from device</span>
    <span class="n">broadcast</span><span class="p">(</span><span class="n">host_in</span><span class="p">,</span> <span class="n">dev</span><span class="p">.</span><span class="n">begin</span><span class="p">());</span>
    <span class="n">copy</span><span class="p">(</span><span class="n">dev</span><span class="p">,</span> <span class="n">host_out</span><span class="p">.</span><span class="n">begin</span><span class="p">());</span>
    <span class="n">synchronize_barrier</span><span class="p">();</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span><span class="o">&lt;</span><span class="n">e</span><span class="p">.</span><span class="n">size</span><span class="p">();</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
      <span class="n">printf</span><span class="p">(</span><span class="s">&quot;in: %f out %f</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span> <span class="n">host_in</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">host_out</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
  <span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The host vector is only of size 1. It&#8217;s content is broadcasted to the segmented
device vector: each segment will contain the same data after the broadcast. The
data is then gathered to host memory to access the result.</p>
</div>
<div class="section" id="distributed-batched-ffts">
<h2><a class="toc-backref" href="#id6">Distributed Batched FFTs</a><a class="headerlink" href="#distributed-batched-ffts" title="Permalink to this headline">¶</a></h2>
<p>Multi-GPU systems are well suited to parallelize batched FFT plans. The MGPU
library provides facilities to automatically distribute FFT plans across all
available devices. Single FFTs can currently not be split across devices but it
works well with batches of FFTs. The following example illustrates how 2D FFTs
are distributed across devices.</p>
<div class="highlight-c++"><div class="highlight"><pre><span class="cp">#include &lt;vector&gt;</span>

<span class="cp">#include &lt;mgpu/fft.hpp&gt;</span>
<span class="cp">#include &lt;mgpu/container/seg_dev_vector.hpp&gt;</span>
<span class="cp">#include &lt;mgpu/transfer/copy.hpp&gt;</span>
<span class="cp">#include &lt;mgpu/synchronization.hpp&gt;</span>

<span class="k">using</span> <span class="k">namespace</span> <span class="n">mgpu</span><span class="p">;</span>

<span class="kt">int</span> <span class="n">main</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
  <span class="n">environment</span> <span class="n">e</span><span class="p">;</span>
  <span class="p">{</span>
    <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">dim</span> <span class="o">=</span> <span class="mi">128</span><span class="p">;</span>
    <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">batch</span> <span class="o">=</span> <span class="mi">15</span><span class="p">;</span>

    <span class="n">std</span><span class="o">::</span><span class="n">size_t</span> <span class="n">blocksize</span> <span class="o">=</span> <span class="n">dim</span><span class="o">*</span><span class="n">dim</span><span class="p">;</span>
    <span class="n">std</span><span class="o">::</span><span class="n">size_t</span> <span class="n">size</span> <span class="o">=</span> <span class="n">blocksize</span><span class="o">*</span><span class="n">batch</span><span class="p">;</span>

    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">complex</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="o">&gt;</span> <span class="n">host_in</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">complex</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(</span><span class="mi">0</span><span class="p">));</span>
    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">complex</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="o">&gt;</span> <span class="n">host_out</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">complex</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(</span><span class="mi">0</span><span class="p">));</span>

    <span class="n">std</span><span class="o">::</span><span class="n">generate</span><span class="p">(</span><span class="n">host_in</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">host_in</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span> <span class="n">rand</span><span class="p">);</span>

    <span class="n">seg_dev_vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">complex</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="o">&gt;</span> <span class="n">in</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">blocksize</span><span class="p">);</span>
    <span class="n">seg_dev_vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">complex</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="o">&gt;</span> <span class="n">out</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">blocksize</span><span class="p">);</span>

    <span class="n">copy</span><span class="p">(</span><span class="n">host_in</span><span class="p">,</span> <span class="n">in</span><span class="p">.</span><span class="n">begin</span><span class="p">());</span>

    <span class="c1">// plan 2D FFT batch with dimension and batch</span>
    <span class="n">fft</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">complex</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">complex</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="o">&gt;</span> <span class="n">f</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">batch</span><span class="p">);</span>

    <span class="n">f</span><span class="p">.</span><span class="n">forward</span><span class="p">(</span><span class="n">in</span><span class="p">,</span> <span class="n">out</span><span class="p">);</span>
    <span class="n">f</span><span class="p">.</span><span class="n">inverse</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">in</span><span class="p">);</span>

    <span class="c1">// fetch result</span>
    <span class="n">copy</span><span class="p">(</span><span class="n">in</span><span class="p">,</span> <span class="n">host_out</span><span class="p">.</span><span class="n">begin</span><span class="p">());</span>
    <span class="n">synchronize_barrier</span><span class="p">();</span>
  <span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>15 2D FFTs each of size 128*128 are calculated in parallel on as many devices
as are available in the system. Input and output buffers are first allocated
on the host. Vectors of the same size but segmented and with the proper
blocksize (the size of one 2D FFT) are then allocated on the devices. A FFT plan
is created - it is implicitly segmented. The forward and inverse functions can
be called to calculate the actual transform.</p>
</div>
</div>


          </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="rationale.html" title="Rationale"
             >next</a> |</li>
        <li class="right" >
          <a href="gettingstarted.html" title="Getting Started"
             >previous</a> |</li>
        <li><a href="index.html">mgpu v0.2 alpha documentation</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2011, Sebastian Schaetz.
      Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 1.0.7.
    </div>
  </body>
</html>